{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Machine Learning Project's Life Cycle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will describe a custom Cross Industry Standart Process for Machine Learning (CRISP-ML) lifecycle in my ML projects.\n",
    "The framework of ML project lifecycle steps are defined in the figure with backtracking below:\n",
    "\n",
    "![ML Life Cycle](image-20220613-061022.png \"ML Life Cycle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!**\n",
    "\n",
    "Each step must go over a quality assurance procedure to ensure that errors are caught as early as possible to minimize costs in the later stages of the development."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "1. [Business Understanding]()\n",
    "    1. Define business objectives\n",
    "    2. Translate business objectives into ML objectives\n",
    "    3. Collect and verify the raw/input data\n",
    "    4. Assess the project feasibility\n",
    "    5. Agree on the scope and timeline for a POC \n",
    "2. [Literature & Best Practices Review]()\n",
    "    1. Define the important criteria of the project to filter out less related works\n",
    "    2. Search for open source products and SaaS providers for similar objectives\n",
    "    3. Look out only for mature projects, academic literature and practical tutorials \n",
    "    4. Make a pros/cons table for all methods with their summaries and comparisons \n",
    "    5. Make a subset of chosen methods and reasonings\n",
    "3. [Data Understanding]()\n",
    "    1. Talk to the data analyst of the product to understand their data sources including their collection frequency and all limitations\n",
    "    2. Verify the integrity and quality; save the criteria and indices of the parts where the data is more/less reliable (in terms of both rows and columns)\n",
    "    3. Do not aggregate the data in this phase yet and focus on the more detailed granular levels\n",
    "    4. Create a data catalog with explained data source and constraints\n",
    "    5. Push the data with individual tables to DVC with `raw` tag\n",
    "4. [Data Preparation & Preprocessing]()\n",
    "    1. Feature selection (columns)\n",
    "    2. Data-Sample selection (rows)\n",
    "    3. Class balancing\n",
    "    4. Cleaning data (noise reduction, missing data imputation, normalisation, outlier detection etc.)\n",
    "    5. Feature engineering  \n",
    "    6. Data augmentation\n",
    "    7. Data standardization \n",
    "    8. Merging and aggregating the data in preparation of the final data set. Pushing it to DVC with `dataset` tag\n",
    "    9. Split data (train, val, test), and push indices to DVC\n",
    "5. [Modelling]()\n",
    "    1. Setting an (MLflow) environment for experiments: Documenting the trials\n",
    "    2. Define quality measure of the model\n",
    "    3. Baseline model selection\n",
    "    4. Adding domain knowledge to specialize the model\n",
    "    5. Model training\n",
    "    6. Optional: using transfer learning with a pre-trained model\n",
    "    7. Model compression\n",
    "    8. Ensemble learning\n",
    "6. [Evaluation]()\n",
    "    1. Validate the model's performance\n",
    "    2. Determine robustness\n",
    "    3. Increase model's explainability\n",
    "    4. Make a decision whether to deploy the model\n",
    "    5. Document the evaluation criteria \n",
    "7. [Production & Deployment]()\n",
    "    1. Evaluate model once more on real world data in production conditions\n",
    "    2. Setup continuous integration pipeline for re-training, versioning, and deployment of model\n",
    "    3. Specify the deployment strategy to be used (A/B testing, multi-armed bandits) \n",
    "    4. Assure user acceptance and usability\n",
    "    5. Define procedures for model governance to monitor while maintenance\n",
    "8. [Monitoring & Maintenance]()\n",
    "    1. Monitor the efficiency and efficacy of the deployed model's predictions \n",
    "    2. Create a dashboard tracks the predefined success criteria (e.g. quality thresholds)\n",
    "    3. Retrain the model if/when required on new data\n",
    "    4. Perform labelling of the new data points\n",
    "    5. Repeat tasks from the _modeling_ and _evaluation_ steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1. [The Paper](https://arxiv.org/pdf/2003.05155.pdf)\n",
    "1. [Andrew NG's course on Coursera](https://www.coursera.org/learn/introduction-to-machine-learning-in-production)\n",
    "1. https://christophergs.com/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/\n",
    "1. https://ml-ops.org/content/crisp-ml\n",
    "1. https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
